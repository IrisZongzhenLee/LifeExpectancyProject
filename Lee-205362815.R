###########################################################################
##
# name: Zongzhen Lee   UID: 205362815
# Homework 9 project
# programs perform 3 analysis (kmean, PCA, MLE) on national Life expectancy
# from year 2010 and year 2015
##
###########################################################################

## load data of national life expectancy source from Kaggle, numbers from WHO
## 20 variables, 157 observations with data from year 2010
data.10=read.csv("data10.csv", header=T)
data.10=data.10[,-1] # remove index

## 20 variables, 157 observations with data from year 2015
data.15=read.csv("data15.csv", header=T)
data.15=data.15[,-1] # remove index

#######################################
# study 1: year 2010 life expectancy ##
#######################################

################# (a) Section 1: K-means algorithms #######################
X=data.10[,-(1:4)]
head(X)
Xc=scale(X,scale=T)
head(Xc)   # scale data
# Find the further distance initial points
head(dist(Xc)) # not that useful since dataset to large
# lets use life expectancy to find our furthest point
#min life expectancy
match(min(Xc[,1]), Xc[,1]) # returns observation 22: Sierra Leone
match(max(Xc[,1]), Xc[,1]) # returns observation 120: Spain

# Initial means for the two variables in cluster 1 (values of obs 22)
c1= Xc[22,]#initial center for cluster 1
#Initial means for the two variables in cluster 2 (values of obs 120)
c2= Xc[120,] #initial center for cluster 2 

#Two index vectors (Z vectors): one  will tell us clusters assigned in last iteration
# Another index vector will tell cluster in the new iteration
#If the two vectors are not the same, we are not done, we 
# must continue allocating. 
# We make the indicator vectors as different as possible to start. 
pastIndicator=157:1   #initial value for z 
indicator=1:157    # past indicator will be compared with new indicator
### note: we initialize this way to get the algorithm started

###### We must iterate until pastIndicator=indicator
## While the two indicator vectors are different, keep going. 
while(sum(pastIndicator!=indicator)!=0) {
  pastIndicator=indicator; 
  #distance to current cluster centers
  dc1 =colSums((t(Xc)-c1)^2)
  dc2=colSums((t(Xc)-c2)^2)
  dMat=matrix(c(dc1,dc2),,2)
  ##decide which cluster each point belongs to##
  indicator = max.col(-dMat)
  # update the cluster centers
  c1=colMeans(Xc[indicator==1,])
  c2=colMeans(Xc[indicator==2,])
}
clusters.kmeans.algorithm.X=pastIndicator
clusters.kmeans.algorithm.X

##### analysis
# cluster1 summary
cluster1=X[clusters.kmeans.algorithm.X==1,]
head(cluster1)
mean.cluster1=apply(cluster1,2,mean)
mean.cluster1       # mean for cluster1
median.cluster1=apply(cluster1,2,median)
median.cluster1     # median for cluster1
var.cluster1=apply(cluster1,2,var)
sd.cluster1=sqrt(var.cluster1)
sd.cluster1

# cluster 2 summary
cluster2=X[clusters.kmeans.algorithm.X==2,]
head(cluster2)
mean.cluster2=apply(cluster2,2,mean)
mean.cluster2       # mean for cluster2
median.cluster2=apply(cluster2,2,median)
median.cluster2     # median for cluster1
var.cluster2=apply(cluster2,2,var)
sd.cluster2=sqrt(var.cluster2)
sd.cluster2

# cluster composition
region=factor(data.10[,3])
prop.table(table(clusters.kmeans.algorithm.X,region), 2)  # gives us the percent composition of clusters by regions

# plot for variables
# life expectancy vs GNI
plot(X[,1], X[,16],
     col=c("red","blue")[unclass(clusters.kmeans.algorithm.X)],pch=c  
     (23,24)[unclass  
             (clusters.kmeans.algorithm.X)],main="Life Expectancy vs GNI per Capita by clusters"
     ,xlab="Life Expectancy", ylab="Gross National Income per Capita")
legend("topleft",c("cluster 1","cluster 2"),pch=c(23,24), col=c("red","blue"))

# life expectancy vs Alcohol
plot(X[,1], X[,6],
     col=c("red","blue")[unclass(clusters.kmeans.algorithm.X)],pch=c  
     (23,24)[unclass  
             (clusters.kmeans.algorithm.X)],main="Life Expectancy vs Alcohol Consumption by clusters"
     ,xlab="Life Expectancy", ylab="Alcohol Consumption (in liter)")
legend("topleft",c("cluster 1","cluster 2"),pch=c(23,24), col=c("red","blue")) 

################# (b) Section 2: PC analysis #################
## First find variance covariance matrix of centered and scaled data
X=data.10[,-(1:4)]
head(X)
Xc.var=var(Xc)
head(Xc.var)

## Then find the eigenvalues (variances of the PC) and the eigenvectors (the axes of
## the projection)
EP=eigen(Xc.var) 

lambda=EP$values

Proportion.of.variance.of.each.pc=100*(lambda/sum(lambda))
Proportion.of.variance.of.each.pc
cumsum(Proportion.of.variance.of.each.pc)
### Find the eigenvectors
V=EP$vectors
#Compute the matrix of principal components scores (the coordinates 
## of the data in the space generated by the eigenvectors )
PC=Xc%*%V    
head(PC)
cor(PC)  # check that there is no correlation between PC

# PC vs. variables
cor(X, PC)

### Prepare for plot ###
PCframe=data.frame(PC,data.10[,2])
head(PCframe)
# par(mfrow=c(2,1))
plot(jitter(PCframe[,1]),jitter(PCframe[,2]), xlab="PC1",ylab="PC2",
     main="Plot of the values of the PC variables") 
text(PCframe[,1],PCframe[,2],labels=PCframe[,17],col=c("red"))
abline(h=0)
abline(v=0)

# This is nice, but it would be nice to 
# see which variables are in which
# direction. The biplot is a plot that shows 
## both the original variable vectors and the 
## observations (you combine variable space and 
## observation space in one plot). We use a function in 
## R that does PC automatically and can produce the plot 
## directly. Just to see what it looks like. 

pca.results=prcomp(Xc)
print(pca.results)
biplot(pca.results,xlabs=data.10[,2])
abline(h=0); abline(v=0)

################# (c) Section 3: MLE optimization ##################
# the variable I choose here is Life Expectancy
x=data.10[,5]
head(x)  # life expectancy of 157 countries
hist(x)  # estimate what kind of probabilistic model the dataset fits
# seems like normal distribution would fit well

# use Newton algorithm to find the MLE of the parameters and asymptotic confidence intervals for the parameters
# parameters: mu and sigma squared
# from program 1 using nlm() we can approx. range for x1 and x2
x1=seq(60,80,by=0.1) # values of x1 (mu)
x2=seq(45,80,by=0.1)  # values of x2(sigma^2)
n=length(x)
f=matrix(0, nrow=length(x1), ncol=length(x2))  # empty matrix for pairs of x1 and x2
for (i in 1:length(x1)) {
  for (j in 1:length(x2)){
    f[i,j]= -n/2*log(2*pi)-n/2*log(x2[j])-1/(2*x2[j])*sum((x-x1[i])^2)
  }
}
# contour plot
contour(x1,x2,f,nlevels=30, xlab="mu",ylab="sigma")
# here we sees optimization at around mu = 70, sigma = 8.5
########## numerical method ############
#### start defining gradient and hessian
xt=c(100,0)       # this helps us get started; meaningless
eps=0.000000001 # tolerance for norm of (xtp1-xt) more recent minus last
xtp1=c(70, 65)  # vector with initial value for x1 and x2

# recall from previous program, our function is
# f=-(n/2)*log(xtp1[2])-(n/2)*(log(2*pi))-(1/(2*xtp1[2]))*sum((data-xtp1[1])^2)
# perform some algebraic derivation to obtain gradient and hessian
# gradient=as.vector(c((1/xt[2])*(sum(data-xt[1])) , 
#                     -n/(2*xt[2]) + 1/(2*(xt[2]^(2)))*sum((data -xt[1])^2))) # our gradient
# hessian=matrix(c(-n/xt[2],(-1/xt[2]^2)*(sum(data-xt[1])),(-1/xt[2]^2)*(sum(data-xt[1])), n/(2*xt[2]^2)-(1/(xt[2]^3))*sum((data-xt[1])^2) ),ncol=2,nrow=2) # our hessian
xHist=matrix(xtp1,2,1) # save history of xt
xHist                # history of parameter vectors

##### Newton Raphson method on function given ######
while(abs(sum((xtp1-xt)^2)) > eps){
  xt=xtp1
  xt
  gradient=as.vector(c((1/xt[2])*(sum(x-xt[1])),
                       -n/(2*xt[2]) + 1/(2*(xt[2]^(2)))*sum((x-xt[1])^2)))
  gradient
  hessian=matrix(c(-n/xt[2],(-1/xt[2]^2)*(sum(x-xt[1])),(-1/xt[2]^2)*(sum(x-xt[1])),n/(2*xt[2]^2)-(1/(xt[2]^3))*sum((x-xt[1])^2)),
                 ncol=2,nrow=2)
  hessian
  ###
  # compute xtp1 solve(hessian*gradient=hessian^{-1}*gradient)
  ###
  xtp1=xt-solve(hessian)%*%gradient  # Newton iteration
  xtp1
  ###
  #save history
  xHist=matrix(c(xHist,xtp1),2)
}
xHist
######## results ########
newton_mle_mu=xHist[, ncol(xHist)][1]
newton_mle_mu   # final estimate for mu using newtons
newton_mle_sigma_squared=xHist[, ncol(xHist)][2]
newton_mle_sigma_squared    # final estimate for sigma squared using newtons

newton_std=sqrt(diag(solve(-hessian)))
newton_std     # final standard errors for mu and sigma using newtons

########## confidence interval ##########
CI_newton_mu=newton_mle_mu + c(-1,1)*qnorm(0.95)*newton_std[1]
CI_newton_mu   # confidence interval for mu at 95% confidence level
CI_newton_sigma_squared=newton_mle_sigma_squared + c(-1,1)*qnorm(0.95)*newton_std[2]
CI_newton_sigma_squared    # confidence interval for signa squared at 95% confidence level


#######################################
# study 2: year 2015 life expectancy ##
#######################################
################# (a) Section 1: K-means algorithms #######################
Y=data.15[,-(1:4)]
head(Y)
Yc=scale(Y,scale=T)
head(Yc)   # scale data
# Find the further distance initial points
head(dist(Yc)) # not that useful since dataset to large
# lets use life expectancy to find our furthest point
#min life expectancy
match(min(Yc[,1]), Yc[,1]) # returns observation 22: Sierra Leone
match(max(Yc[,1]), Yc[,1]) # returns observation 120: Spain

# Initial means for the two variables in cluster 1 (values of obs 22)
c1= Yc[22,]#initial center for cluster 1
#Initial means for the two variables in cluster 2 (values of obs 120)
c2= Yc[120,] #initial center for cluster 2 

#Two index vectors (Z vectors): one  will tell us clusters assigned in last iteration
# Another index vector will tell cluster in the new iteration
#If the two vectors are not the same, we are not done, we 
# must continue allocating. 
# We make the indicator vectors as different as possible to start. 
pastIndicator=157:1   #initial value for z 
indicator=1:157    # past indicator will be compared with new indicator
### note: we initialize this way to get the algorithm started

###### We must iterate until pastIndicator=indicator
## While the two indicator vectors are different, keep going. 
while(sum(pastIndicator!=indicator)!=0) {
  pastIndicator=indicator; 
  
  #distance to current cluster centers
  dc1 =colSums((t(Yc)-c1)^2)
  dc2=colSums((t(Yc)-c2)^2)
  dMat=matrix(c(dc1,dc2),,2)
  
  #decide which cluster each point belongs to 
  indicator = max.col(-dMat)
  
  # update the cluster centers
  c1=colMeans(Yc[indicator==1,])
  c2=colMeans(Yc[indicator==2,])
  
}
clusters.kmeans.algorithm.Y=pastIndicator
clusters.kmeans.algorithm.Y

#a nalysis
# cluster1 summary
cluster1=Y[clusters.kmeans.algorithm.Y==1,]
head(cluster1)
mean.cluster1=apply(cluster1,2,mean)
mean.cluster1       # mean for cluster1
median.cluster1=apply(cluster1,2,median)
median.cluster1     # median for cluster1
var.cluster1=apply(cluster1,2,var)
sd.cluster1=sqrt(var.cluster1)
sd.cluster1

# cluster 2 summary
cluster2=Y[clusters.kmeans.algorithm.Y==2,]
head(cluster2)
mean.cluster2=apply(cluster2,2,mean)
mean.cluster2       # mean for cluster2
median.cluster2=apply(cluster2,2,median)
median.cluster2     # median for cluster1
var.cluster2=apply(cluster2,2,var)
sd.cluster2=sqrt(var.cluster2)
sd.cluster2

# cluster composition
region=factor(data.15[,3])
prop.table(table(clusters.kmeans.algorithm.X,region), 2)  # gives us the percent composition of clusters by regions


#plot for variables
# life expectancy vs GNI
plot(Y[,1], Y[,16],
     col=c("darkgreen","orange")[unclass(clusters.kmeans.algorithm.Y)],pch=c  
     (23,24)[unclass  
             (clusters.kmeans.algorithm.Y)],main="Life Expectancy vs GNI per Capita by clusters"
     ,xlab="Life Expectancy", ylab="Gross National Income per Capita")
legend("topleft",c("cluster 1","cluster 2"),pch=c(23,24), col=c("darkgreen","orange"))
# life expectancy vs Alcohol
plot(Y[,1], Y[,6],
     col=c("darkgreen","orange")[unclass(clusters.kmeans.algorithm.Y)],pch=c  
     (23,24)[unclass  
             (clusters.kmeans.algorithm.Y)],main="Life Expectancy vs Alcohol Consumption by clusters"
     ,xlab="Life Expectancy", ylab="Alcohol Consumption (in liter)")
legend("topleft",c("cluster 1","cluster 2"),pch=c(23,24), col=c("darkgreen","orange")) 

################# (b) Section 2: PC analysis #######################
## First find variance covariance matrix of centered and scaled data
Y=data.15[,-(1:4)]
head(Y)
Yc.var=var(Yc)
head(Yc.var)

## Then find the eigenvalues (variances of the PC) and the eigenvectors (the axes of
## the projection)
EP=eigen(Yc.var) 

lambda=EP$values

Proportion.of.variance.of.each.pc=100*(lambda/sum(lambda))
Proportion.of.variance.of.each.pc
cumsum(Proportion.of.variance.of.each.pc)
### Find the eigenvectors
V=EP$vectors
#Compute the matrix of principal components scores (the coordinates 
## of the data in the space generated by the eigenvectors )
PC=Yc%*%V    
head(PC)
cor(PC)  # check that there is no correlation between PC

# PC vs. variables
cor(Y, PC)

### Prepare for plot ###
PCframe=data.frame(PC,data.15[,2])
head(PCframe)
# par(mfrow=c(2,1))
plot(jitter(PCframe[,1]),jitter(PCframe[,2]), xlab="PC1",ylab="PC2",
     main="Plot of the values of the PC variables") 
text(PCframe[,1],PCframe[,2],labels=PCframe[,17],col=c("red"))
abline(h=0)
abline(v=0)

#This is nice, but it would be nice to 
# see which variables are in which
#direction. The biplot is a plot that shows 
## both the original variable vectors and the 
## observations (you combine variable space and 
## observation space in one plot). We use a function in 
## R that does PC automatically and can produce the plot 
## directly. Just to see what it looks like. 

pca.results=prcomp(Yc)
print(pca.results)
biplot(pca.results,xlabs=data.15[,2])
abline(h=0); abline(v=0)

################# (c) Section 3: MLE optimization #######################
# the variable I choose here is Life Expectancy
x=data.15[,5]
head(x)  # life expectancy of 157 countries
hist(x)  # estimate what kind of probabilistic model the dataset fits
# seems like normal distribution would fit well

# use Newton algorithm to find the MLE of the parameters and asymptotic confidence intervals for the parameters
# parameters: mu and sigma
# from program 1 using nlm() we can approx. range for x1 and x2
x1=seq(60,80,by=0.1) # values of x1 (mu)
x2=seq(45,80,by=0.1)  # values of x2(sigma^2)
n=length(x)
f=matrix(0, nrow=length(x1), ncol=length(x2))  # empty matrix for pairs of x1 and x2
for (i in 1:length(x1)) {
  for (j in 1:length(x2)){
    f[i,j]= -n/2*log(2*pi)-n/2*log(x2[j])-1/(2*x2[j])*sum((x-x1[i])^2)
  }
}
# contour plot
contour(x1,x2,f,nlevels=30, xlab="mu",ylab="sigma")
# here we sees optimization at around mu = 72, sigma = 55
########## numerical method ############
#### start defining gradient and hessian
xt=c(100,0)       # this helps us get started; meaningless
eps=0.000000001 # tolerance for norm of (xtp1-xt) more recent minus last
xtp1=c(72, 55)  # vector with initial value for x1 and x2

# recall from previous program, our function is
# f=-(n/2)*log(xtp1[2])-(n/2)*(log(2*pi))-(1/(2*xtp1[2]))*sum((data-xtp1[1])^2)
# perform some algebraic derivation to obtain gradient and hessian
# gradient=as.vector(c((1/xt[2])*(sum(data-xt[1])) , 
#                     -n/(2*xt[2]) + 1/(2*(xt[2]^(2)))*sum((data -xt[1])^2))) # our gradient
# hessian=matrix(c(-n/xt[2],(-1/xt[2]^2)*(sum(data-xt[1])),(-1/xt[2]^2)*(sum(data-xt[1])), n/(2*xt[2]^2)-(1/(xt[2]^3))*sum((data-xt[1])^2) ),ncol=2,nrow=2) # our hessian
xHist=matrix(xtp1,2,1) # save history of xt
xHist                # history of parameter vectors

##### Newton Raphson method on function given ######
while(abs(sum((xtp1-xt)^2)) > eps){
  xt=xtp1
  xt
  gradient=as.vector(c((1/xt[2])*(sum(x-xt[1])),
                       -n/(2*xt[2]) + 1/(2*(xt[2]^(2)))*sum((x-xt[1])^2)))
  gradient
  hessian=matrix(c(-n/xt[2],(-1/xt[2]^2)*(sum(x-xt[1])),(-1/xt[2]^2)*(sum(x-xt[1])),n/(2*xt[2]^2)-(1/(xt[2]^3))*sum((x-xt[1])^2)),
                 ncol=2,nrow=2)
  hessian
  ###
  # compute xtp1 solve(hessian*gradient=hessian^{-1}*gradient)
  ###
  xtp1=xt-solve(hessian)%*%gradient  # Newton iteration
  xtp1
  ###
  #save history
  xHist=matrix(c(xHist,xtp1),2)
}
xHist
######## results ########
newton_mle_mu=xHist[, ncol(xHist)][1]
newton_mle_mu   # final estimate for lambda using newtons
newton_mle_sigma_squared=xHist[, ncol(xHist)][2]
newton_mle_sigma_squared    # final estimate for theta using newtons

newton_std=sqrt(diag(solve(-hessian)))
newton_std     # final standard errors for lambda and theta using newtons

########## confidence interval ##########
CI_newton_mu=newton_mle_mu + c(-1,1)*qnorm(0.95)*newton_std[1]
CI_newton_mu   # confidence interval for lambda at 95% confidence level
CI_newton_sigma_squared=newton_mle_sigma_squared + c(-1,1)*qnorm(0.95)*newton_std[2]
CI_newton_sigma_squared    # confidence interval for theta at 95% confidence level

